\chapter{Analysis and Design}
This chapter describes all the changes and additions made to Issie by this project, as well as organisational decisions. Additionally, the rationale behind these changes, including analysis of high-level methods, will be explained. 
\section{Approach towards Software Development}
Features were added to Issie using an incremental and Agile approach \cite{Voorhees2020}.
The incremental approach seeks to write code through a repeated cycle of three steps: (1) Analysis and Design, (2) Writing Code, and (3) Testing. A basic task/requirement is  broken into several parts - with each of these parts being written as an individual function \footnote{"individual function" does not refer to a single F\fsharp function, but to a top-level function which uses groups of helper and sub-functions}. Each function is tested both as a unit and when integrated into the codebase. All of these different parts build upon one another and come together to deliver the desired functionality. One caveat of the incremental approach is that the intermediate versions of the app are incomplete and therefore not suitable for any kind of release or proper demonstration. This can make it difficult to get proper feedback on the state of the application as a whole. This was acceptable within short time-frames, but not for long-term software development over the course of the project. For that case, the Agile approach was considered. Small features, represented as a short sequence of stories, were built using the aforementioned incremental approach during sprints. Upon completion of each feature, it could be considered that a new "product" (slightly improved version of Issie) was delivered. During each project meeting, feedback was obtained on the work completed, and any necessary adjustments will spawned new user stories. Agile is generally used in continuous software development projects; however, due to the constraints of a Final Year Project: deadlines, need for planning and report writing, a pure Agile approach was not deemed not suitable. Instead, a hybrid approach was pursued, one which embodies Agile principles while still working within a plan-based framework. During the planning phase, the backlog of user stories was intelligently structured such that epics were ordered by importance to the project, and stories within the epics were structured such that features would be added incrementally.

\section{Technology Stack}
This project does not make any deviations from Issie's existing technology stack, described in Section \ref{sec:techstack}. The F\fsharp application code is compiled to JavaScript using the Fable compiler, and is run as a web app in a desktop environment using Electron. The user interface is powered by React and the MVU architecture, brought to the F\fsharp ecosystem by the \codestyle{Fable.React} and \codestyle{Elmish} libraries. New files added to the Issie codebase by this project are split between two existing directories in the \textit{Renderer} project. Files which implement features related to actual truth table generation and reduction are placed in the \textit{Simulator} directory, while files which implement the UI are placed in the \textit{UI} directory. 

\input{04.AnalysisDesign/overview}

\section{Considering Logic Input with Truth Tables}
In addition to visualising combinational logic, the possibility of using truth tables as a method for combinational logic input was also considered. Issie in its current form only allows for combinational logic input via the canvas; components and connections are manually arranged by the user into a valid schematic. 
The goal of the digital electronics and computer architecture curriculum at Imperial College is to first build up studentsâ€™ understanding of digital circuit design using schematics, and then eventually transition to the use of Hardware Description Languages (HDLs) in subsequent modules. Component-level schematics tend to focus on the propagation of digital signals through multiple components, while HDLs describe circuits at a behavioural level, focusing on the relationships between inputs and outputs. Defining logic using truth tables could possibly bridge the learning gap between these two concepts; prompting users to shift their design approach towards the more abstracted view of functionally mapping inputs to outputs, while also providing a familiar environment (truth tables instead of Verilog) for defining those functions.

The following system was envisaged: users would be able to define a custom hierarchical component by defining the truth table for that component. This process would begin by the user providing the names and widths of each input and output on the custom component. Using the provided inputs, the input space (left hand side) for the component's truth table would be calculated and displayed to the user with a blank right hand side. For small input spaces, the user could manually enter the output corresponding to each input row. For larger input spaces, the user would be prompted to set certain inputs as algebra, and populate the outputs with algebraic expressions which were a function of the inputs. The arguments for and against implementing this system were considered, and are presented in Table \ref{tab:logicinput}. While there was a concern that giving users a way to directly define components may detract from them learning schematic design, it was decided that from an educational perspective the system would improve Issie. However, certain issues with the implementation of algebraic truth tables for logic input were identified. A GUI would have to be developed for users to enter algebraic expressions; these would then have to be lexed and parsed. This activity was estimated to take a significant amount of time. Additionally, the algebraic relationships would have to be converted to Verilog. It was deemed that the educational benefit brought by implementing logic creation through user-entered truth truth tables was outweighed by the time investment necessary to implement the system, and that more tangible returns could be achieved by focusing that time elsewhere.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|m{7cm}|m{7cm}|}
    \hline
        Pros & Cons \\ \hline
        Can help bridge the conceptual gap between component-level design and more abstracted HDL design patterns. & While gate-level design and HDLs are widely used in industry, Issie's algebraic truth table system is not standard. Therefore, using it may not transfer well into later education or industry. \\ \hline
        Would decrease the time spent designing specific components, as specific gate level operations need not be considered. This improves the user experience. & Decreasing the number of times students build gate-level designs could itself detract from their learning experience. \\ \hline
        Flexible, users can choose to define all cases or use algebraic expressions to define the component. For example, inputs which control certain parameters of operation can be left as numeric values, while operands to functions can be defined as algebraic expressions. & A GUI for entering truth tables will have to be created. Additionally, to support algebra a GUI, as well as a lexer and syntax checker will have to be created -- this is will take a significant amount of time. If algebra is not implemented, truth table definitions may only be used to define less complex components, decreasing the effectiveness of the system. \\ \hline
        ~ & Issie schematics currently can be exported as Verilog. Therefore, a function for converting a truth table component to Verilog would have to be written. This is straightforward for pure numeric truth tables, but challenging for algebraic truth tables. \\ \hline
    \end{tabular}
    \caption{Arguments for and against implementing truth-table defined custom components}
    \label{tab:logicinput}
\end{table}



% \subsection{Generating Numeric Truth Tables}
% The primary aim of the project was the generation of truth tables with interactive features to aid in logic visualisation. Thus truth table generation lies at the is a key part of the application logic. When the user first generates a truth table by clicking on the \textit{Generate Truth Table} button, a numerical truth table is generated and shown to them.
% At its core, generating a complete numeric truth table is a brute-force process. Each possible combination of input values must be simulated to calculate the corresponding output values, and the relationship should be recorded in the table. 
% Given that Issie already features a performant and reliable simulator (step simulator) for calculating the outputs of combinational logic, the decision was made to use as much of its implementation as possible. This approach has many advantages:
% \begin{itemize}
%     \item The existing step simulator has been extensively tested by end-users, meaning that its implementation is most likely bug-free. By using it, it will reduce the chance of the new feature introducing new bugs.
%     \item In most cases, reusing existing code is much faster than writing new code from scratch. Not only is time saved on writing new code, but the amount of time spent debugging is also reduced.
%     \item Reusing existing code will help keep the overall size of the codebase small. Not only does this help future programmers who work on the project by reducing how much they have to understand, but it also means that any future improvements made to the simulation code are also improvements to Truth Table generation.
% \end{itemize}

% Figure \ref{fig:flowchartSim} provides an overview of Issie's process for building and running a Step Simulation. In this process, various checks must be performed; firstly the logic designed by the user must be verified to be syntactically correct, secondly the organisation of project files must be correct, and thirdly some Issie specific limitations (e.g. no cycles in combinational logic) must be enforced. Issie's simulation building process can be said to have three levels, with each level having an associated data structure which represents schematic. These data structures are: the \textbf{Canvas State}, \textbf{Simulation Data/Error}, and a \textbf{Fast Simulation}. A set of checks are performed at each level, and only upon passing these checks can a schematic be transformed to the subsequent data structure.
% If any of these checks fail due to an issue with the user's schematic, the simulation building process returns a \textit{SimulationError}, which tells the user what the error is and which components/connections are affected. A key takeaway from Figure \ref{fig:flowchartSim} is that the process of building a simulation is separate to the process of running it. During the \codestyle{FastSimulation} building process, the schematic is analysed, with components being placed into an appropriate order for combinational reduction. However, as the \codestyle{FastSimulation} data structure is mutable, the values of each input can be updated without having to rebuild the whole simulation. Therefore, the time taken simulating a different input combination is quite short, as only the reduction function has to be re-run to find the new outputs. 
% This distinction between building a simulation and running it with different input values makes the existing Step Simulator an optimal choice for use in truth table generation, which will need to simulate multiple input combinations as fast as possible. 

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{04.AnalysisDesign/IssieSim.pdf}
%     \caption{An overview of Issie's Step Simulation}
%     \label{fig:flowchartSim}
% \end{figure}

% Figure \ref{fig:ttGen} shows a high-level view of truth table generation in Issie. A simulation for the sheet is built using data stored in the current application state -- when generating truth tables for the whole sheet, this process is the same as the one which happens when starting the Step Simulator. When generating a truth table for a partial selection of a sheet, a few extra steps are required prior to the simulation building logic being called. The Step Simulator informs users of the correctness of their schematic through colour coded buttons. If the schematic is correct, the user will be shown a green button which allows them to start a simulation. On the other hand, if there are mistakes in the schematic, the user will instead be shown the \textit{See Problems} button, which will explain the nature of the Simulation Error. This functionality is replicated for the truth table, as it clearly informs the user of errors in their schematic. An additional check is also implemented for truth tables; the schematic must \textbf{only contain combinational logic}.

% If the checks for correctness and combinational logic pass, truth table generation can proceed. Alongside the Simulation Data, the truth table generation logic also requires information such as the input constraints and which inputs are algebraic. In Issie, truth tables are initially generated without any constraints or algebraic inputs, and are re-generated when these are added. Therefore, when the user clicks the button, these parameters are empty. From this information, the input space of the truth table (its left-hand side) is determined. Each input combination is then simulated, with the output and viewer values corresponding to that combination recorded and stored. A truth table is simply the mapping between input and output combinations - this mapping is stored so that it may later be viewed in tabular form by the View function.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{04.AnalysisDesign/ttGen.pdf}
%     \caption{High-Level view of Truth Table Generation}
%     \label{fig:ttGen}
% \end{figure}

% The process of calculating the input space, highlighted in purple in Figure \ref{fig:ttGen} had two versions; the change from the first to the second affected the overall structuring of operations on truth tables in the application. Initially, the entire input space was calculated and simulated. This would result in a very large, exhaustive truth table -- one which contained every possible input and output combination. The user would haven been able to reduce this truth table by filtering the table or using Don't Care Reduction. However, several issues arose with this approach, with the most crucial being that of performance. The complexity of finding the Cartesian product of multiple sets is an exponential function of the number of sets. For example, the complexity of finding the Cartesian product of $n$ sets, each of size $s$, would be $O(s^n)$. The size of each set is dependent on the width of the input; Issie does not limit input widths, meaning that the size of the sets may also be very large. For example, a single 16-bit input has 65536 unique values, and combining this with a second 16-bit input yields an input space of over 4 billion unique combinations. Simulating this large number of combinations and storing the result takes upwards of 20 seconds, violating essential requirement \textbf{E1.7}. Additionally, a sufficiently large input space could cause the system to run out of memory and crash, which is totally unacceptable. Along with the practical issues with the generation of the complete truth table, it can also be argued that a very large complete truth table is not useful to the user. According to the Atkinson-Shiffrin memory model described in Section \ref{subsec:memmodels}, external stimuli only stay in the sensory register for 0.5 seconds prior to decaying. A large number of rows is likely to overload the sensory register, meaning that it is likely that the information will not pass to short term memory unless the the user slows down and processes each row one-by-one. Even if this were the case, a truth table with a large number of rows would take over 30 seconds for the user to read and process. Given that short term memory decays in 30 seconds, this means that the user will likely have forgotten the first row by the time they finish reading the last one. Therefore, the method of generating and simulating the entire input space to create an exhaustive truth table was deemed unfit.

% As a result, the decision was made to \textbf{truncate the truth table}; generate and simulate only part of the input space. The current limit is 1024 rows. Considering that users would only be able to focus on a small subset of rows at a time, and would likely reduce the size of the table anyways, this approach would sacrifice very little for a significant gain. However, any subsequent operations, such as filtering and reduction, would take place on the truncated table, which does not contain all of the necessary data. The nature of these issues, as well as the steps taken to mitigate them are:

% \begin{itemize}
%     \item[] \textbf{Issue 1}: Input constraints will be applied to the truncated table, so user may not see a full representation of the relationships for the case they enter. For example, consider the input $X$ which has a width of 8 bits (so values range from 0 to 255), but due to truncation, only rows with values of $X$ up to 31 are present. If the user applies the input constraint $X = 32$ to the table, an empty table will be returned as no rows which fulfil this condition exist in the truncated table.
%     \item[] \textbf{Solution 1}: Apply input constraints during truth table generation. This is achieved by using the constraints to determine a tighter input space, and then simulating that. Giving users a way to choose which inputs contribute more to the input space (they can fix certain inputs and let others vary within bounds) allows them to interactively generate truth tables that deliver the most information to them.
%     \item [] \textbf{Issue 2}: Output constraints will be applied to the truncated table, which is missing many rows. Due to this, the result of applying the output constraints will include all rows which match the condition.
%     \item [] \textbf{Solution 2}: Unfortunately there is not much that can be done to combat this issue alone other than warn the user that they are looking at incomplete results. However, if the user is able to use input constraints to sufficiently reduce the input space first, output constraints could help filter the table further.
%     \item [] \textbf{Issue 3}: Don't Care Reduction will occur on the truncated table, which does not fully represent the logical function performed by the circuit. This could result in incorrect relationships being inferred by the reduction algorithm.
%     \item [] \textbf{Solution 3} Much like with output constraints, there is no perfect solution as relationships for reduction are inferred from the truth table. Don't Care reduction is therefore limited to smaller schematics which do not produce truncated truth tables. The user is instead guided towards reducing truth tables for larger schematics with \textbf{Algebraic Reduction}.
% \end{itemize}

% Implementing Solution 1 required a new method of generating the input space; a general view of the method used is shown in Figure \ref{fig:tableLHS}. As generating input combinations is expensive in terms of time and memory, the input space generation method needs to know which sub-sets of the input space it can and cannot generate before it actually generates it using the Cartesian product method. Further details of how this is implemented can be found in the Implementation chapter, with elaboration on the complex \codestyle{inputsWithARC} function (highlighted purple on Figure \ref{fig:tableLHS}).
% \begin{figure}
%     \centering
%     \includegraphics{04.AnalysisDesign/newLHS.pdf}
%     \caption{High-Level view of how the Input Space is generated}
%     \label{fig:tableLHS}
% \end{figure}